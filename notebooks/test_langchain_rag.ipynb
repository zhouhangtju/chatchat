{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5fd03d-c5f0-499f-9f05-4f966ceddc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_by_key(tuple_list):\n",
    "    return list({t[0]: t for t in tuple_list}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77de0958-73a9-4923-b060-41de27ede379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'c'), (2, 'b'), (3, 'd')]\n"
     ]
    }
   ],
   "source": [
    "original_list = [(1, 'a'), (2, 'b'), (1, 'c'), (3, 'd')]\n",
    "deduplicated_list = deduplicate_by_key(original_list)\n",
    "print(deduplicated_list)  # 输出: [(2, 'b'), (1, 'c'), (3, 'd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4131bf51-76e8-4f7b-8c9c-f7f051e2857d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"1\":2,\"2\":2,\"2\":3}.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8e9d93-38ee-4f93-b479-2abe82ea1163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/llm/Langchain-Chatchat-master\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "from pathlib import Path\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "print(current_dir)\n",
    "sys.path.append(current_dir)\n",
    "sys.path.append(Path(current_dir).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9427d449-b6ef-460a-83b4-bcdc51ea46d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"no_proxy\"]=\"127.0.0.1\"\n",
    "print(os.getenv(\"no_proxy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7431c26f-470e-49a8-962f-dc14b51c5316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/conda/envs/chatchat/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/root/.local/conda/envs/chatchat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-27 16:33:45,159 - SentenceTransformer.py[line:66] - INFO: Load pretrained SentenceTransformer: /root/model/BAAI/bge-large-zh-v1.5\n",
      "/root/.local/conda/envs/chatchat/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "2024-07-27 16:33:50,922 - loader.py[line:54] - INFO: Loading faiss with AVX2 support.\n",
      "2024-07-27 16:33:50,991 - loader.py[line:56] - INFO: Successfully loaded faiss with AVX2 support.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.76it/s]\n",
      "/root/.local/conda/envs/chatchat/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/root/.local/conda/envs/chatchat/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from server.knowledge_base.kb_service.base import EmbeddingsFunAdapter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "embeddings = EmbeddingsFunAdapter()\n",
    "class KnowledgeBaseSelector:\n",
    "    def __init__(self, kb_list: List[str], llm: ChatOpenAI):\n",
    "        self.kb_list = kb_list\n",
    "        self.llm = llm\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"kb_list\"],\n",
    "            template=\"\"\"Given the following query and list of knowledge bases, select the most relevant knowledge bases for answering the query. Return the names of the selected knowledge bases as a comma-separated list.\n",
    "Query: {query}\n",
    "Knowledge Bases: {kb_list}\n",
    "Relevant Knowledge Bases:\"\"\"\n",
    "        )\n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt, output_parser=CommaSeparatedListOutputParser())\n",
    "\n",
    "    def select_kbs(self, query: str) -> List[str]:\n",
    "        print(\"#\")\n",
    "        return self.chain.run(query=query, kb_list=\", \".join(self.kb_list))\n",
    "\n",
    "    \n",
    "kb_list = [\"history\", \"technology\", \"science\", \"literature\"]\n",
    "\n",
    "# Initialize components\n",
    "from configs import (LLM_MODELS, LLM_DEVICE, EMBEDDING_DEVICE,\n",
    "                     MODEL_PATH, MODEL_ROOT_PATH, ONLINE_LLM_MODEL, logger, log_verbose,\n",
    "                     FSCHAT_MODEL_WORKERS, HTTPX_DEFAULT_TIMEOUT)\n",
    "from server.utils import get_model_worker_config,fschat_openai_api_address\n",
    "config = get_model_worker_config(\"llama3\")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"llama3-8b\",temperature=0,openai_api_key=config.get(\"api_key\", \"EMPTY\"),openai_api_base=config.get(\"api_base_url\", fschat_openai_api_address()))\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "# Create knowledge bases\n",
    "kb_texts = {\n",
    "    \"history\": [\"Historical text 1\", \"Historical text 2\"],\n",
    "    \"technology\": [\"Tech text 1\", \"Tech text 2\"],\n",
    "    \"science\": [\"Science text 1\", \"Science text 2\"],\n",
    "    \"literature\": [\"Literature text 1\", \"Literature text 2\"]\n",
    "}\n",
    "\n",
    "kb_dict = {name: FAISS.from_texts(texts, embeddings, metadatas=[{\"source\": name}]*len(texts)) \n",
    "           for name, texts in kb_texts.items()}\n",
    "# print(kb_dict)\n",
    "# Initialize KnowledgeBaseSelector and CustomRetriever\n",
    "kb_selector = KnowledgeBaseSelector(kb_list, llm)\n",
    "# retriever = CustomRetriever(kb_dict, embeddings)\n",
    "\n",
    "def process_query(query: str) -> str:\n",
    "    # Select relevant knowledge bases\n",
    "    selected_kbs = kb_selector.select_kbs(query)\n",
    "    print(f\"Selected knowledge bases: {selected_kbs}\")\n",
    "    print(\"end\")\n",
    "\n",
    "#     # Retrieve relevant documents\n",
    "#     retrieved_docs = retriever.retrieve(query, selected_kbs)\n",
    "    \n",
    "#     # Generate answer using retrieved documents\n",
    "#     context = \"\\n\".join([f\"From {doc['kb']}: {doc['content']}\" for doc in retrieved_docs])\n",
    "#     answer_prompt = PromptTemplate(\n",
    "#         input_variables=[\"query\", \"context\"],\n",
    "#         template=\"Answer the following query based on the provided context:\\n\\nQuery: {query}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "#     )\n",
    "#     answer_chain = LLMChain(llm=llm, prompt=answer_prompt)\n",
    "#     answer = answer_chain.run(query=query, context=context)\n",
    "\n",
    "#     return answer\n",
    "\n",
    "# Example usage\n",
    "query = \"What are some significant technological advancements in the 20th century?\"\n",
    "result = process_query(query)\n",
    "# print(f\"Query: {query}\")\n",
    "# print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd562f-0878-4553-bbd0-71545791ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fd39b-4203-43d9-8d10-a6db8ee86f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatchat",
   "language": "python",
   "name": "chatchat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
